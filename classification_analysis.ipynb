{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load, Loader\n",
    "from bunch import Bunch\n",
    "Bunch.__str__ = Bunch.__repr__\n",
    "\n",
    "stream = open(\"config.yaml\", 'r')\n",
    "config = Bunch(load(stream, Loader=Loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotting_util as putil\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "SIGNAL_COMBOS = [['driver_behavior', 'vehicle_behavior'], ['driver_behavior', 'vehicle_behavior', 'navi'],\n",
    "                ['driver_behavior', 'vehicle_behavior', 'radar'], ['driver_behavior', 'vehicle_behavior', 'navi', 'radar']]\n",
    "\n",
    "metrics = ['test_balanced_accuracy', 'train_balanced_accuracy', 'test_roc_auc', 'train_roc_auc']\n",
    "\n",
    "can_data = pd.read_parquet('out/can_data.parquet', columns=['subject_id'])\n",
    "subject_ids = np.unique(can_data['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare window sizes\n",
    "scores = []\n",
    "for window_size in config.window_sizes:\n",
    "    results = pd.read_csv('out/results/{}_{}_pred_results_windowsize_{}.csv'.format(\n",
    "                config.classifier_type, config.clf_mode, window_size), index_col=0, usecols=lambda x: x not in ['fit_time', 'score_time'])\n",
    "    score = results.loc[subject_ids, metrics]\n",
    "    scores.append(score)\n",
    "scores = np.array(scores)\n",
    "for i in range(scores.shape[2]):\n",
    "    fig, axes = putil.create_plot(nrows=1, ncols=1, sharey=True)\n",
    "    for ax in axes:\n",
    "        putil.set_ax_visible_spines(ax, top=False, right=False)\n",
    "    putil.set_figure_size(fig, 5, 5)\n",
    "    axes[0].boxplot(np.transpose(scores[:, :, i]))\n",
    "    putil.set_ax_xticks(axes[0], range(1, scores.shape[0]+1), config.window_sizes)\n",
    "    putil.set_ax_yticks(axes[0], np.linspace(0.5, 1, 5), np.linspace(0.5, 1, 5))\n",
    "    putil.set_ax_axis_labels(axes[0], x_label='window sizes', y_label=[s.replace('_', ' ') for s in metrics][i])\n",
    "    putil.set_ax_bg_color(axes[0], 'whitesmoke')\n",
    "    putil.set_ax_grid_lines(axes[0], True, axis=\"y\", color=\"w\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('out/results/{}_{}_scores_varying_window_sizes_{}.png'.format(config.classifier_type, config.clf_mode, metrics[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare signal combos\n",
    "mean_scores = []\n",
    "stddevs = []\n",
    "window_size = 60\n",
    "for combo in SIGNAL_COMBOS:\n",
    "    signal_string = ''\n",
    "    for signal in combo:\n",
    "        signal_string += '_' + signal\n",
    "    results = pd.read_csv('out/results/{}_{}_pred_results_windowsize_{}{}.csv'.format(\n",
    "                config.classifier_type, config.clf_mode, window_size, signal_string), index_col=0, usecols=lambda x: x not in ['fit_time', 'score_time'])\n",
    "    mean_score = results.loc[subject_ids, scores]\n",
    "    stddev = results.loc['stddev', scores]\n",
    "    mean_scores.append(mean_score)\n",
    "    stddevs.append(stddev)\n",
    "mean_scores = np.array(mean_scores)\n",
    "stddevs = np.array(stddevs)\n",
    "for i in range(mean_scores.shape[1]):\n",
    "    axes[1].plot(range(len(mean_scores)), mean_scores[:, i], markerfacecolor='white', marker=\"o\", label=scores[i])\n",
    "    axes[1].fill_between(range(len(mean_scores)), y1=mean_scores[:, i]-stddevs[:, i], y2=mean_scores[:, i]+stddevs[:, i], alpha=0.2)\n",
    "putil.set_ax_xticks(axes[1], range(len(mean_scores)), SIGNAL_COMBOS, rotation=90)\n",
    "putil.set_ax_axis_labels(axes[1], 'signal combos')\n",
    "putil.set_ax_bg_color(axes[1], 'whitesmoke')\n",
    "putil.set_ax_grid_lines(axes[1], True, axis=\"y\", color=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare step sizes\n",
    "scores = []\n",
    "window_size = 120\n",
    "for overlap_percentage in config.overlap_percentages:\n",
    "    step = 1\n",
    "    if overlap_percentage is not None:\n",
    "        step = window_size - int(overlap_percentage * window_size)\n",
    "        results = pd.read_csv('out/results/{}_{}_pred_results_step_size_{}_windowsize_{}.csv'.format(\n",
    "                    config.classifier_type, config.clf_mode, step, window_size), index_col=0, usecols=lambda x: x not in ['fit_time', 'score_time'])\n",
    "        score = results.loc[subject_ids, metrics]\n",
    "        scores.append(score)\n",
    "scores = np.array(scores)\n",
    "for i in range(scores.shape[2]):\n",
    "    fig, axes = putil.create_plot(nrows=1, ncols=1, sharey=True)\n",
    "    for ax in axes:\n",
    "        putil.set_ax_visible_spines(ax, top=False, right=False)\n",
    "    putil.set_figure_size(fig, 5, 5)\n",
    "    axes[0].boxplot(np.transpose(scores[:, :, i]))\n",
    "    putil.set_ax_xticks(axes[0], range(1, scores.shape[0]+1), [window_size - int(overlap_percentage * window_size) for overlap_percentage in config.overlap_percentages])\n",
    "    putil.set_ax_yticks(axes[0], np.linspace(0.5, 1, 5), np.linspace(0.5, 1, 5))\n",
    "    putil.set_ax_axis_labels(axes[0], x_label='step sizes', y_label=[s.replace('_', ' ') for s in metrics][i])\n",
    "    putil.set_ax_bg_color(axes[0], 'whitesmoke')\n",
    "    putil.set_ax_grid_lines(axes[0], True, axis=\"y\", color=\"w\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('out/results/{}_{}_scores_varying_step_sizes_{}.png'.format(config.classifier_type, config.clf_mode, metrics[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotting_util as putil\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "SIGNAL_COMBOS = [['driver_behavior', 'vehicle_behavior'], ['driver_behavior', 'vehicle_behavior', 'navi'],\n",
    "                ['driver_behavior', 'vehicle_behavior', 'radar'], ['driver_behavior', 'vehicle_behavior', 'navi', 'radar']]\n",
    "\n",
    "SCENARIOS = ['highway', 'rural', 'town']\n",
    "\n",
    "scores = ['test_balanced_accuracy', 'train_balanced_accuracy', 'test_roc_auc', 'train_roc_auc']\n",
    "\n",
    "fig, axes = putil.create_plot(nrows=1, ncols=len(SIGNAL_COMBOS), sharey=True)\n",
    "for ax in axes:\n",
    "    putil.set_ax_visible_spines(ax, top=False, right=False)\n",
    "putil.set_figure_size(fig, 25, 5)\n",
    "\n",
    "colors = ['blue', 'royalblue', 'green', 'limegreen']\n",
    "# compare scenarios\n",
    "for i, scenario in enumerate(SCENARIOS):\n",
    "    for j, window_size in enumerate(config.window_sizes):\n",
    "        for k, combo in enumerate(SIGNAL_COMBOS):\n",
    "            mean_scores = []\n",
    "            mean_stddev = []\n",
    "            signal_string = ''\n",
    "            for signal in combo:\n",
    "                signal_string += '_' + signal\n",
    "            results = pd.read_csv('out/results/{}_{}_pred_results_windowsize_{}_step_size_1s{}_{}.csv'.format(\n",
    "                            config.classifier, config.clf_mode, window_size, signal_string, scenario), index_col=0, usecols=lambda x: x not in ['fit_time', 'score_time'])\n",
    "            mean_scores.append(results.loc['mean', scores])\n",
    "            mean_stddev.append(results.loc['stddev', scores])\n",
    "            mean_scores = np.array(mean_scores).mean(axis=0)\n",
    "            mean_stddev = np.array(mean_stddev).mean(axis=0)\n",
    "            width = 0.5\n",
    "            frac = width/len(scores)\n",
    "            for l in range(len(scores)):\n",
    "                if l < len(scores) / 2:\n",
    "                    b = axes[k].bar(i+1 - (l+1)*frac, mean_scores[l], frac, yerr=mean_stddev[l], label=scores[l], color=colors[l], align='edge', capsize=5)\n",
    "                    axes[k].bar_label(b, fmt='%.2f')\n",
    "                else:\n",
    "                    b = axes[k].bar(i+1 + (l-(len(scores)//2))*frac, mean_scores[l], frac, yerr=mean_stddev[l], label=scores[l], color=colors[l], align='edge', capsize=5)\n",
    "                    axes[k].bar_label(b, fmt='%.2f')\n",
    "            putil.set_ax_title(axes[k], combo)\n",
    "            putil.set_ax_xticks(axes[k], range(1, len(SCENARIOS)+1), SCENARIOS)\n",
    "            putil.set_ax_axis_labels(axes[k], x_label='scenarios', y_label='score')\n",
    "            putil.set_ax_bg_color(axes[k], 'whitesmoke')\n",
    "            putil.set_ax_grid_lines(axes[k], True, axis=\"y\", color=\"w\")\n",
    "plt.legend(scores, bbox_to_anchor=(1.0, 1.0))\n",
    "putil.set_fig_title(fig, title='Scores per scenario (window size: {}s)'.format(window_size), font_size=14, font_weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('out/results/{}_{}_{}_scores_per_scenario_and_signal_combo.png'.format(config.classifier, config.dataset, config.clf_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotting_util as putil\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "SCORING = ['balanced_accuracy', 'roc_auc', 'f1_micro', 'average_precision', 'recall', 'precision']\n",
    "\n",
    "PARAMETERS = {'n_estimators': [50, 100, 150], \"max_features\": ['sqrt', 'log2']}\n",
    "\n",
    "SIGNAL_COMBOS = [['driver_behavior', 'vehicle_behavior'], ['driver_behavior', 'vehicle_behavior', 'navi'],\n",
    "                ['driver_behavior', 'vehicle_behavior', 'radar'], ['driver_behavior', 'vehicle_behavior', 'navi', 'radar']]\n",
    "\n",
    "SCENARIOS = ['highway', 'rural', 'town']\n",
    "\n",
    "top_n = 5\n",
    "fig, axes = putil.create_plot(len(config.window_sizes) * len(SIGNAL_COMBOS), len(SCENARIOS))\n",
    "putil.set_figure_size(fig, 40, 30)\n",
    "putil.set_fig_xlabel(fig, 'Coefficient', font_size=14)\n",
    "putil.set_fig_ylabel(fig, 'Top {} features'.format(top_n), font_size=14)\n",
    "for i, window_size in enumerate(config.window_sizes):\n",
    "    for j, combo in enumerate([SIGNAL_COMBOS[-1]]):\n",
    "        signal_string = ''\n",
    "        can_data_features = []\n",
    "        for signal in combo:\n",
    "            signal_string += '_' + signal\n",
    "            can_data_features.append(pd.read_parquet('out/can_data_features_{}_windowsize_{}s.parquet'.format(signal, window_size)))\n",
    "        can_data_features = pd.concat(can_data_features, axis=1)\n",
    "\n",
    "        for k, scenario in enumerate(SCENARIOS):\n",
    "            print('signals: {}, window size: {}s, step size: {}s ({}), scenario: {}'.format(\n",
    "                signal_string, window_size, step, overlap_percentage, scenario\n",
    "                ))\n",
    "                \n",
    "            if config.classifier == 'log_regression':\n",
    "                coefficients = []\n",
    "                for ind in range(len(subject_ids)):\n",
    "                    est = cv['estimator'][ind]['logisticregression']\n",
    "                    coefficients.append(est.coef_[0])\n",
    "                coefficients = pd.DataFrame(coefficients, columns=can_data_features_bin.columns.to_list()[:-1])\n",
    "                top_features = coefficients.mean(axis=0).abs().nlargest(top_n).index.to_list()\n",
    "                ax = axes[i*len(SIGNAL_COMBOS) + j][k]\n",
    "                if i*len(SIGNAL_COMBOS) + j == 0:\n",
    "                    putil.set_ax_title(ax, scenario)\n",
    "                sns.swarmplot(ax=ax, data=coefficients.loc[:, top_features], orient='h', size=2)\n",
    "                putil.set_ax_grid_lines(ax, flag=True, style=':', axis='y', color='lightgrey')\n",
    "                putil.set_ax_visible_spines(ax, False, False, True, False)              \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('out/results/{}_coef_top_{}.png'.format(config.classifier, top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotting_util as putil\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "EVENTS = ['brake', 'brake_to_gas', 'gas', 'gas_to_brake', 'overtaking', 'road_sign', 'turning']\n",
    "\n",
    "scores = ['test_balanced_accuracy', 'train_balanced_accuracy', 'test_roc_auc', 'train_roc_auc']\n",
    "\n",
    "SCENARIOS = ['highway', 'rural', 'town']\n",
    "\n",
    "fig, axes = putil.create_plot(nrows=1, ncols=1, sharey=True)\n",
    "for ax in axes:\n",
    "    putil.set_ax_visible_spines(ax, top=False, right=False)\n",
    "putil.set_figure_size(fig, 5, 5)\n",
    "\n",
    "avg_scores = []\n",
    "avg_stddev = []\n",
    "for scenario in SCENARIOS:\n",
    "    results = pd.read_csv('out/results/{}_pred_results_events_{}.csv'.format(\n",
    "                config.classifier, scenario), index_col=0, usecols=lambda x: x not in ['fit_time', 'score_time'])\n",
    "    avg_scores.append(results.loc['mean', scores])\n",
    "    avg_stddev.append(results.loc['stddev', scores])\n",
    "avg_scores = np.array(avg_scores)\n",
    "avg_stddev = np.array(avg_stddev)\n",
    "for i in range(avg_scores.shape[1]):\n",
    "    axes[0].plot(range(len(SCENARIOS)), avg_scores[:, i], markerfacecolor='white', marker=\"o\", label=scores[i])\n",
    "    axes[0].fill_between(range(len(SCENARIOS)), y1=avg_scores[:, i]-avg_stddev[:, i], y2=avg_scores[:, i]+avg_stddev[:, i], alpha=0.2)\n",
    "putil.set_ax_xticks(axes[0], range(len(SCENARIOS)), SCENARIOS)\n",
    "putil.set_ax_axis_labels(axes[0], x_label='scenarios', y_label='score')\n",
    "putil.set_ax_bg_color(axes[0], 'whitesmoke')\n",
    "putil.set_ax_grid_lines(axes[0], True, axis=\"y\", color=\"w\")\n",
    "\n",
    "plt.legend()\n",
    "putil.set_fig_title(fig, title='Scores for all events combined', font_size=14, font_weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('out/results/{}_{}_scores_events_combined.png'.format(config.classifier, config.clf_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotting_util as putil\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "EVENTS = ['brake', 'brake_to_gas', 'gas', 'gas_to_brake', 'overtaking', 'road_sign', 'turning']\n",
    "\n",
    "scores = ['test_balanced_accuracy', 'train_balanced_accuracy', 'test_roc_auc', 'train_roc_auc']\n",
    "\n",
    "SCENARIOS = ['highway', 'rural', 'town']\n",
    "\n",
    "fig, axes = putil.create_plot(nrows=1, ncols=len(EVENTS), sharey=True)\n",
    "for ax in axes:\n",
    "    putil.set_ax_visible_spines(ax, top=False, right=False)\n",
    "putil.set_figure_size(fig, 25, 5)\n",
    "\n",
    "for i, event in enumerate(EVENTS):\n",
    "    avg_scores = []\n",
    "    avg_stddev = []\n",
    "    for scenario in SCENARIOS:\n",
    "        if event == 'turning' and scenario == 'highway':\n",
    "            avg_scores.append([0, 0, 0, 0])\n",
    "            avg_stddev.append([0, 0, 0, 0])\n",
    "        else:\n",
    "            results = pd.read_csv('out/results/{}_{}_pred_results_{}_{}.csv'.format(\n",
    "                        config.classifier, config.clf_mode, event, scenario), index_col=0, usecols=lambda x: x not in ['fit_time', 'score_time'])\n",
    "            avg_scores.append(results.loc['mean', scores])\n",
    "            avg_stddev.append(results.loc['stddev', scores])\n",
    "    avg_scores = np.array(avg_scores)\n",
    "    avg_stddev = np.array(avg_stddev)\n",
    "    for j in range(avg_scores.shape[1]):\n",
    "        axes[i].plot(range(len(SCENARIOS)), avg_scores[:, j], markerfacecolor='white', marker=\"o\", label=scores[j])\n",
    "        axes[i].fill_between(range(len(SCENARIOS)), y1=avg_scores[:, j]-avg_stddev[:, j], y2=avg_scores[:, j]+avg_stddev[:, j], alpha=0.2)\n",
    "    putil.set_ax_xticks(axes[i], range(len(SCENARIOS)), SCENARIOS)\n",
    "    putil.set_ax_title(axes[i], event)\n",
    "    putil.set_ax_bg_color(axes[i], 'whitesmoke')\n",
    "    putil.set_ax_grid_lines(axes[i], True, axis=\"y\", color=\"w\")\n",
    "\n",
    "plt.legend()\n",
    "putil.set_fig_title(fig, title='Scores per event', font_size=14, font_weight='bold')\n",
    "putil.set_fig_xlabel(fig, 'scenarios')\n",
    "putil.set_fig_ylabel(fig, 'score')\n",
    "plt.tight_layout()\n",
    "plt.savefig('out/results/{}_{}_scores_per_event.png'.format(config.classifier, config.clf_mode))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb6cecb4b017aacc75af7fc465a2b65ff55fcf20de66591fd1094dae1cba1d54"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('DRIVE-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
