{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load, Loader\n",
    "from bunch import Bunch\n",
    "\n",
    "stream = open(\"config.yaml\", 'r')\n",
    "config = Bunch(load(stream, Loader=Loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import shap\n",
    "import random\n",
    "\n",
    "SCORING = ['roc_auc', 'accuracy', 'balanced_accuracy', 'f1_micro', 'average_precision', 'recall', 'precision']\n",
    "PARAMETERS = {'n_estimators': [50, 100, 150], \"max_features\": ['sqrt', 'log2']}\n",
    "\n",
    "SIGNAL_COMBOS = [['driver_behavior'], ['driver_behavior', 'vehicle_behavior'], \n",
    "            ['driver_behavior', 'vehicle_behavior', 'navi'], ['driver_behavior', 'vehicle_behavior', 'navi', 'radar']]\n",
    "\n",
    "def collect_scores(scoring, y_true, y_pred):\n",
    "    scores_dict = dict()\n",
    "    for scorer in scoring:\n",
    "        s = get_scorer(scorer)\n",
    "        scores_dict[scorer] = s._score_func(y_true, y_pred)\n",
    "    return scores_dict\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), PCA(), LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, class_weight='balanced'))\n",
    "\n",
    "for window_size in config.window_sizes:\n",
    "    for combo in SIGNAL_COMBOS:\n",
    "        signal_string = ''\n",
    "        can_data_features = []\n",
    "        for signal in combo:\n",
    "            signal_string += '_' + signal\n",
    "            can_data_features.append(pd.read_parquet('out/can_data_features_{}_windowsize_{}s.parquet'.format(signal, window_size)))\n",
    "        can_data_features = pd.concat(can_data_features, axis=1)\n",
    "        can_data_features.loc[:, 'label'] = 0\n",
    "        can_data_features.loc[(slice(None), 'above', slice(None)), 'label'] = 1\n",
    "\n",
    "        can_data_features.replace(np.nan, 10e6, inplace=True)\n",
    "\n",
    "        # drop below BAC level for binary classification\n",
    "        can_data_features = can_data_features.drop('below', level=1)\n",
    "\n",
    "        for scenario in ['highway', 'rural', 'town']:\n",
    "            print('signals: {}, window size: {}s, scenario: {}'.format(signal_string, window_size, scenario))\n",
    "\n",
    "            can_data_features_bin = can_data_features.loc[:, :, scenario, :]\n",
    "\n",
    "            groups = list(can_data_features_bin.index.get_level_values('subject_id'))\n",
    "            subject_ids = np.unique(groups)\n",
    "            \n",
    "            X = can_data_features_bin.drop(columns='label').to_numpy()\n",
    "            \n",
    "            y = can_data_features_bin['label'].to_numpy()\n",
    "\n",
    "            cv = cross_validate(estimator=pipeline, X=X, y=y, scoring=SCORING, return_estimator=True,\n",
    "                    return_train_score=True, cv=LeaveOneGroupOut(), groups=groups, n_jobs=None)\n",
    "\n",
    "            ind = random.choice(range(len(subject_ids)))\n",
    "            feature_names = can_data_features_bin.columns.to_list()[:-1]\n",
    "            explainer = shap.LinearExplainer(cv['estimator'][ind]['logisticregression'], X, feature_names=feature_names)\n",
    "            shap_values = explainer.shap_values(X)\n",
    "\n",
    "            shap_values = pd.DataFrame(shap_values)\n",
    "            shap_values.columns = feature_names\n",
    "            shap_values.set_index(can_data_features_bin.index).to_parquet(\n",
    "                'out/shap_values_windowsize_{}{}_{}.parquet'.format(window_size, signal_string, scenario), index=True\n",
    "            )\n",
    "\n",
    "            exclude = ['estimator']\n",
    "            \n",
    "            pd.DataFrame({k:v for k,v in cv.items() if k not in exclude}).set_index(subject_ids).to_csv(\n",
    "                    'out/pred_results_windowsize_{}{}_{}.csv'.format(window_size, signal_string, scenario), index=True, header=True\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d38b0f11020d666728afea22b8c28c9b5047b48a579ca7bb4bf92d3abe21fb0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
